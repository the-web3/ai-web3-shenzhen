# Web3 RAG 系统设计文档

> **版本**: v1.0  
> **更新日期**: 2026-01-25

---

## 1. 整体架构设计

### 1.1 系统架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Web3 RAG 智能问答系统                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      表现层 (Presentation Layer)                      │   │
│  │  ┌───────────────────────────────────────────────────────────────┐  │   │
│  │  │                    Next.js Frontend (:3000)                    │  │   │
│  │  │  ┌─────────────┐  ┌──────────────┐  ┌───────────────────────┐ │  │   │
│  │  │  │ Chat Input  │  │   Message    │  │   Source References   │ │  │   │
│  │  │  │    Box      │  │   Stream     │  │   (Collapsible)       │ │  │   │
│  │  │  └─────────────┘  └──────────────┘  └───────────────────────┘ │  │   │
│  │  └───────────────────────────────────────────────────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                      │ HTTP/SSE                             │
│                                      ▼                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      服务层 (Service Layer)                          │   │
│  │  ┌───────────────────────────────────────────────────────────────┐  │   │
│  │  │                   FastAPI Backend (:8080)                      │  │   │
│  │  │  ┌─────────────┐  ┌──────────────┐  ┌───────────────────────┐ │  │   │
│  │  │  │ POST /chat  │  │ GET /health  │  │    CORS Middleware    │ │  │   │
│  │  │  │ (Stream)    │  │              │  │                       │ │  │   │
│  │  │  └─────────────┘  └──────────────┘  └───────────────────────┘ │  │   │
│  │  └───────────────────────────────────────────────────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                      │                                      │
│               ┌──────────────────────┴──────────────────────┐              │
│               ▼                                              ▼              │
│  ┌──────────────────────────┐              ┌──────────────────────────┐    │
│  │       RAG Engine         │              │    LlamaFactory API      │    │
│  │      (LlamaIndex)        │─────────────▶│       (:8000)            │    │
│  │  ┌────────────────────┐  │   HTTP POST  │  ┌────────────────────┐  │    │
│  │  │ Document Indexer   │  │   /v1/chat   │  │ Qwen3-4B-Instruct  │  │    │
│  │  │ Vector Retriever   │  │              │  │ + LoRA Adapter     │  │    │
│  │  │ Response Synth     │  │              │  │ (Optional)         │  │    │
│  │  └────────────────────┘  │              │  └────────────────────┘  │    │
│  └──────────────────────────┘              └──────────────────────────┘    │
│               │                                                             │
│               ▼                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      数据层 (Data Layer)                             │   │
│  │  ┌───────────────────┐  ┌──────────────┐  ┌───────────────────────┐ │   │
│  │  │   Knowledge Base  │  │ Vector Index │  │   Fine-tune Data     │ │   │
│  │  │   (PDF/MD/TXT)    │  │   (Chroma)   │  │   (Alpaca JSON)      │ │   │
│  │  └───────────────────┘  └──────────────┘  └───────────────────────┘ │   │
│  │  ┌───────────────────┐  ┌──────────────┐                            │   │
│  │  │   Qwen3-4B        │  │ Qwen3-Embed  │                            │   │
│  │  │   (LLM Weights)   │  │ (4B Weights) │                            │   │
│  │  └───────────────────┘  └──────────────┘                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 部署架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           AutoDL 服务器部署                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        AutoDL Proxy (:6006)                          │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │   │
│  │  │ /web/*      │  │ /api/*      │  │ /v1/*       │  │ /ui/*       │ │   │
│  │  │  → :3000    │  │  → :8080    │  │  → :8000    │  │  → :9090    │ │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌───────────────────────────────┐  ┌───────────────────────────────────┐  │
│  │         GPU 0 (16GB)          │  │          GPU 1 (16GB)             │  │
│  │  ┌─────────────────────────┐  │  │  ┌─────────────────────────────┐  │  │
│  │  │   LlamaFactory API      │  │  │  │      FastAPI Backend        │  │  │
│  │  │   - Qwen3-4B-Instruct   │  │  │  │      - RAG Engine           │  │  │
│  │  │   - LoRA Adapter        │  │  │  │      - Qwen3-Embedding-4B   │  │  │
│  │  │   Port: 8000            │  │  │  │      Port: 8080             │  │  │
│  │  └─────────────────────────┘  │  │  └─────────────────────────────┘  │  │
│  └───────────────────────────────┘  └───────────────────────────────────┘  │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        CPU / 存储                                     │   │
│  │  ┌─────────────────────────┐  ┌─────────────────────────────────┐   │   │
│  │  │    Next.js Frontend     │  │        Data Storage             │   │   │
│  │  │    Port: 3000           │  │  - models/ (gitignored)         │   │   │
│  │  │                         │  │  - data/index_storage/          │   │   │
│  │  │                         │  │  - saves/ (LoRA checkpoints)    │   │   │
│  │  └─────────────────────────┘  └─────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 2. 系统模块划分与职责

### 2.1 模块总览

| 模块 | 路径 | 职责 |
|------|------|------|
| **RAG 核心** | `web3_rag/src/` | 文档处理、向量检索、答案生成 |
| **API 后端** | `web3_rag/app/` | HTTP 接口、CORS、流式响应 |
| **Web 前端** | `web3_rag/frontend/` | 用户界面、消息展示、来源卡片 |
| **模型服务** | LlamaFactory | LLM 推理、LoRA 加载 |
| **配置管理** | `web3_rag/configs/` | 系统参数、模型配置 |
| **数据存储** | `web3_rag/data/` | 知识库、向量索引、训练数据 |
| **脚本工具** | `web3_rag/scripts/` | 索引构建、训练启动、测试 |

### 2.2 RAG 核心模块

```
src/
├── __init__.py
├── rag_engine.py      # 核心：Web3RAGEngine 类
├── llm_client.py      # LlamaFactory API 客户端封装
└── embedding.py       # Qwen3-Embedding-4B 本地加载
```

#### 2.2.1 Web3RAGEngine

**职责**：
- 文档加载与智能分块（按标题/代码块保持上下文完整）
- 向量索引构建与持久化
- 语义检索（Top-K + 相似度阈值过滤）
- 上下文拼接与 LLM 调用
- 双模式回答（学习模式/简洁模式）

**核心方法**：

| 方法 | 功能 |
|------|------|
| `build_index()` | 加载文档、分块、向量化、持久化 |
| `query()` | 简单问答，返回字符串 |
| `chat()` | 带来源的问答，返回结构化结果 |
| `chat_stream()` | 流式问答，支持实时输出 |

#### 2.2.2 智能分块算法

与传统按字数分块不同，本系统采用**结构感知分块**：

```
文档 → 按标题分节 → 按段落/代码块分块 → 字数控制 → 上下文重叠
```

优势：
- 代码块不会被截断
- 标题与内容保持关联
- 技术文档语义完整性更好

### 2.3 API 后端模块

```
app/
├── __init__.py
├── main.py            # FastAPI 入口，生命周期管理
└── api/
    ├── __init__.py
    └── routes.py      # API 路由定义
```

**API 端点**：

| 端点 | 方法 | 功能 |
|------|------|------|
| `/api/chat` | POST | RAG 问答（支持流式） |
| `/api/health` | GET | 健康检查 |

**生命周期管理**：
- 启动时等待 LlamaFactory API 可用
- 预加载 RAG 引擎和向量索引
- 支持 GPU 显存优化（Embedding 可降级 CPU）

### 2.4 Web 前端模块

```
frontend/
├── app/
│   ├── layout.tsx     # 根布局，全局样式
│   ├── page.tsx       # 主聊天页面
│   └── globals.css    # Tailwind + 自定义样式
├── components/
│   ├── ChatInput.tsx  # 输入框组件
│   ├── Message.tsx    # 消息气泡
│   └── ...
├── lib/
│   └── api.ts         # API 客户端，SSE 处理
└── types/
    └── index.ts       # TypeScript 类型定义
```

**核心特性**：
- **流式渲染**：使用 SSE 实时展示 LLM 输出（结束前推送 sources 事件）
- **来源追溯**：可折叠卡片显示引用文档
- **暗色主题**：Web3 风格，紫色渐变 + 毛玻璃效果
- **响应式**：适配桌面和移动端

---

## 3. 关键流程说明

### 3.1 问答流程

```
┌─────────┐     ┌─────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────┐
│  用户   │────▶│ Frontend│────▶│ FastAPI     │────▶│ RAG Engine  │────▶│ LLM API │
│ 输入问题│     │         │     │ /api/chat   │     │             │     │         │
└─────────┘     └─────────┘     └─────────────┘     └─────────────┘     └─────────┘
                                                           │                   │
                                                           │ 1. 向量检索       │
                                                           ▼                   │
                                                    ┌─────────────┐            │
                                                    │ Vector DB   │            │
                                                    │ Top-K 文档  │            │
                                                    └─────────────┘            │
                                                           │                   │
                                                           │ 2. 上下文拼接     │
                                                           ▼                   │
                                                    ┌─────────────┐            │
                                                    │ Prompt      │────────────┘
                                                    │ Template    │  3. LLM 生成
                                                    └─────────────┘
                                                           │
                                                           │ 4. 流式返回
                                                           ▼
                                                    ┌─────────────┐
                                                    │ 用户看到    │
                                                    │ 实时答案    │
                                                    └─────────────┘
```

**详细步骤**：

1. **用户输入**：前端捕获问题，发送 POST 请求
2. **向量检索**：RAG Engine 使用 Qwen3-Embedding 将问题向量化，检索 Top-K 相关文档
3. **相似度过滤**：过滤相似度低于阈值（0.4）的结果
4. **Prompt 构建**：将检索到的上下文 + 问题填入模板
5. **LLM 生成**：调用 LlamaFactory API，流式生成答案
6. **流式返回**：通过 SSE 实时推送到前端

### 3.2 索引构建流程

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ 知识库文档   │────▶│ 文档加载器   │────▶│ 智能分块    │────▶│ 向量化      │
│ PDF/MD/TXT  │     │ PyMuPDF     │     │ 标题/代码块  │     │ Qwen3-Embed │
└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘
                                                                   │
                                                                   ▼
                                                           ┌─────────────┐
                                                           │ Chroma DB   │
                                                           │ 持久化存储   │
                                                           └─────────────┘
```

### 3.3 LoRA 微调流程

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ 知识库文档   │────▶│ Q&A 生成    │────▶│ Alpaca 格式 │────▶│ LoRA 训练   │
│             │     │ 脚本        │     │ JSON        │     │ LlamaFactory│
└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘
                                                                   │
                                                                   ▼
                                                           ┌─────────────┐
                                                           │ LoRA Adapter│
                                                           │ 热加载部署   │
                                                           └─────────────┘
```

---

## 4. 数据流设计

### 4.1 请求数据流

```typescript
// 请求
interface ChatRequest {
  question: string;
  show_sources?: boolean;
  similarity_threshold?: number;
  mode?: "learning" | "concise";
  learner_profile?: string;
  stream?: boolean;
}

// 响应（非流式）
interface ChatResponse {
  answer: string;
  sources: Array<{
    file_name: string;
    text: string;
    score: number;
    page?: number | null;
  }>;
  query_time_ms: number;
  timings: {
    retrieval_ms: number;
    llm_ms: number;
    postprocess_ms: number;
    total_ms: number;
  };
}

// 响应（流式）
// SSE 事件流：
// data: "token"\n\n
// data: {"sources": [...]}\n\n
// data: [DONE]\n\n
```

### 4.2 存储结构

```
data/
├── knowledge_base/           # 知识库文档
│   ├── ethereum_basics.md
│   ├── uniswap_v3.md
│   └── defi_protocols.md
├── index_storage/            # 向量索引（Chroma）
│   ├── docstore.json
│   ├── index_store.json
│   └── vector_store.json
└── finetune/                 # 微调数据
    ├── dataset_info.json     # LlamaFactory 数据集注册
    ├── web3_sft_train.json   # 训练集
    └── web3_sft_eval.json    # 验证集
```

---

## 5. 核心技术选型理由

### 5.1 模型选型

| 组件 | 选型 | 理由 |
|------|------|------|
| **LLM** | Qwen3-4B-Instruct | 中文能力强，4B 参数平衡性能与资源，支持 32K 上下文 |
| **Embedding** | Qwen3-Embedding-4B | 2560 维高质量向量，与 LLM 同系列，语义对齐更好 |

### 5.2 框架选型

| 组件 | 选型 | 理由 |
|------|------|------|
| **LLM 服务** | LlamaFactory | OpenAI 兼容 API，原生支持 LoRA 热加载，支持 vLLM 后端 |
| **RAG 框架** | LlamaIndex | 成熟的检索增强生成框架，丰富的文档解析器 |
| **后端** | FastAPI | 高性能异步框架，原生支持 SSE 流式响应 |
| **前端** | Next.js 14 | React 生态，App Router，服务端渲染 |
| **样式** | TailwindCSS | 原子化 CSS，快速开发暗色主题 |

### 5.3 为什么不用云端 API？

| 考量 | 本地方案优势 |
|------|-------------|
| **隐私** | 查询内容不离开本地，适合企业敏感场景 |
| **成本** | 一次部署，无 API 调用费用 |
| **延迟** | 本地推理，无网络往返延迟 |
| **可控** | 可自定义知识库和微调模型 |
| **离线** | 不依赖网络，断网可用 |

### 5.4 版本兼容性

| 依赖 | 版本约束 | 原因 |
|------|----------|------|
| llama-index-core | 0.11.23 | vLLM 0.11.0 要求 setuptools<80 |
| setuptools | <80 | 与 vLLM 兼容 |
| Python | 3.11+ | LlamaFactory 要求 |
| Node.js | 18+ | Next.js 14 要求 |

---

## 6. 安全与性能设计

### 6.1 安全设计

- **无云端依赖**：所有数据本地处理，无外泄风险
- **输入校验**：FastAPI 自动校验请求参数
- **CORS 配置**：仅允许前端域名访问 API

### 6.2 性能优化

| 优化点 | 实现 |
|--------|------|
| **索引持久化** | 向量库持久化到磁盘，重启秒级加载 |
| **流式响应** | SSE 实时推送，用户无需等待完整生成 |
| **显存分离** | LLM 和 Embedding 可分配到不同 GPU |
| **CPU 降级** | Embedding 支持 CPU 运行，释放 GPU 给 LLM |
| **批量 Embedding** | 支持配置 batch_size，提升吞吐 |

### 6.3 资源需求

| 配置 | 最低要求 | 推荐配置 |
|------|----------|----------|
| **GPU 显存** | 16GB（单卡） | 32GB（双卡分离） |
| **系统内存** | 16GB | 32GB |
| **存储空间** | 20GB（模型） | 50GB（含索引） |

---

## 7. 扩展性设计

### 7.1 知识库扩展

添加新文档只需：
1. 将文件放入 `data/knowledge_base/`
2. 运行 `python scripts/03_build_index.py`

支持格式：PDF、Markdown、TXT

### 7.2 模型切换

修改配置即可切换基座模型：
```yaml
# configs/api_server.yaml
model_name_or_path: ./models/your-new-model
```

### 7.3 LoRA 适配

支持动态加载不同领域的 LoRA 适配器：
```yaml
# configs/api_server_finetuned.yaml
adapter_name_or_path: ./saves/your-domain-lora
```

---

## 8. 未来演进方向

| 方向 | 描述 |
|------|------|
| **多轮对话** | 支持上下文记忆的连续对话 |
| **知识图谱** | 结合 KG 增强实体关系推理 |
| **多模态** | 支持图表、代码截图理解 |
| **Agent 化** | 支持调用链上数据、实时行情 |
| **量化部署** | GPTQ/AWQ 量化，降低显存需求 |

---

## 附录：文件路径速查

| 文件 | 用途 |
|------|------|
| `web3_rag/src/rag_engine.py` | RAG 核心逻辑 |
| `web3_rag/app/main.py` | FastAPI 入口 |
| `web3_rag/frontend/app/page.tsx` | 主页面 UI |
| `web3_rag/configs/api_server.yaml` | LLM API 配置 |
| `web3_rag/configs/train_lora_sft.yaml` | LoRA 训练配置 |
| `start_all.sh` | 一键启动脚本 |
